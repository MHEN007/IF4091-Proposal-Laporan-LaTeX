@inproceedings{10.1145/3035918.3064029,
author = {Van Aken, Dana and Pavlo, Andrew and Gordon, Geoffrey J. and Zhang, Bohan},
title = {Automatic Database Management System Tuning Through Large-scale Machine Learning},
year = {2017},
isbn = {9781450341974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3035918.3064029},
doi = {10.1145/3035918.3064029},
abstract = {Database management system (DBMS) configuration tuning is an essential aspect of any data-intensive application effort. But this is historically a difficult task because DBMSs have hundreds of configuration "knobs" that control everything in the system, such as the amount of memory to use for caches and how often data is written to storage. The problem with these knobs is that they are not standardized (i.e., two DBMSs use a different name for the same knob), not independent (i.e., changing one knob can impact others), and not universal (i.e., what works for one application may be sub-optimal for another). Worse, information about the effects of the knobs typically comes only from (expensive) experience.To overcome these challenges, we present an automated approach that leverages past experience and collects new information to tune DBMS configurations: we use a combination of supervised and unsupervised machine learning methods to (1) select the most impactful knobs, (2) map unseen database workloads to previous workloads from which we can transfer experience, and (3) recommend knob settings. We implemented our techniques in a new tool called OtterTune and tested it on two DBMSs. Our evaluation shows that OtterTune recommends configurations that are as good as or better than ones generated by existing tools or a human expert.},
booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
pages = {1009–1024},
numpages = {16},
keywords = {autonomic computing, database management systems, database tuning, machine learning},
location = {Chicago, Illinois, USA},
series = {SIGMOD '17}
}

@inproceedings{10.1145/3299869.3300085,
author = {Zhang, Ji and Liu, Yu and Zhou, Ke and Li, Guoliang and Xiao, Zhili and Cheng, Bin and Xing, Jiashu and Wang, Yangtao and Cheng, Tianheng and Liu, Li and Ran, Minwei and Li, Zekang},
title = {An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning},
year = {2019},
isbn = {9781450356435},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3299869.3300085},
doi = {10.1145/3299869.3300085},
abstract = {Configuration tuning is vital to optimize the performance of database management system (DBMS). It becomes more tedious and urgent for cloud databases (CDB) due to the diverse database instances and query workloads, which make the database administrator (DBA) incompetent. Although there are some studies on automatic DBMS configuration tuning, they have several limitations. Firstly, they adopt a pipelined learning model but cannot optimize the overall performance in an end-to-end manner. Secondly, they rely on large-scale high-quality training samples which are hard to obtain. Thirdly, there are a large number of knobs that are in continuous space and have unseen dependencies, and they cannot recommend reasonable configurations in such high-dimensional continuous space. Lastly, in cloud environment, they can hardly cope with the changes of hardware configurations and workloads, and have poor adaptability. To address these challenges, we design an end-to-end automatic CDB tuning system, CDBTune, using deep reinforcement learning (RL). CDBTune utilizes the deep deterministic policy gradient method to find the optimal configurations in high-dimensional continuous space. CDBTune adopts a try-and-error strategy to learn knob settings with a limited number of samples to accomplish the initial training, which alleviates the difficulty of collecting massive high-quality samples. CDBTune adopts the reward-feedback mechanism in RL instead of traditional regression, which enables end-to-end learning and accelerates the convergence speed of our model and improves efficiency of online tuning. We conducted extensive experiments under 6 different workloads on real cloud databases to demonstrate the superiority of CDBTune. Experimental results showed that CDBTune had a good adaptability and significantly outperformed the state-of-the-art tuning tools and DBA experts.},
booktitle = {Proceedings of the 2019 International Conference on Management of Data},
pages = {415–432},
numpages = {18},
keywords = {storage, database tuning, cloud},
location = {Amsterdam, Netherlands},
series = {SIGMOD '19}
}

@article{10.14778/3339490.3339503,
author = {Tan, Jian and Zhang, Tieying and Li, Feifei and Chen, Jie and Zheng, Qixing and Zhang, Ping and Qiao, Honglin and Shi, Yue and Cao, Wei and Zhang, Rui},
title = {iBTune: individualized buffer tuning for large-scale cloud databases},
year = {2019},
issue_date = {June 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {10},
issn = {2150-8097},
url = {https://doi.org/10.14778/3339490.3339503},
doi = {10.14778/3339490.3339503},
abstract = {Tuning the buffer size appropriately is critical to the performance of a cloud database, since memory is usually the resource bottleneck. For large-scale databases supporting heterogeneous applications, configuring the individual buffer sizes for a significant number of database instances presents a scalability challenge. Manual optimization is neither efficient nor effective, and even not feasible for large cloud clusters, especially when the workload may dynamically change on each instance. The difficulty lies in the fact that each database instance requires a different buffer size that is highly individualized, subject to the constraint of the total buffer memory space. It is imperative to resort to algorithms that automatically orchestrate the buffer pool tuning for the entire database instances.To this end, we design iBTune that has been deployed for more than 10, 000 OLTP cloud database instances in our production system. Specifically, it leverages the information from similar workloads to find out the tolerable miss ratio of each instance. Then, it utilizes the relationship between miss ratios and allocated memory sizes to individually optimize the target buffer pool sizes.To provide a guaranteed level of service level agreement (SLA), we design a pairwise deep neural network that uses features from measurements on pairs of instances to predict the upper bounds of the request response times. A target buffer pool size can be adjusted only when the predicted response time upper bound is in a safe limit. The successful deployment on a production environment, which safely reduces the memory footprint by more than 17\% compared to the original system that relies on manual configurations, demonstrates the effectiveness of our solution.},
journal = {Proc. VLDB Endow.},
month = jun,
pages = {1221–1234},
numpages = {14}
}

@book{Silberschatz2010,
  added-at = {2010-07-18T14:00:17.000+0200},
  address = {New York},
  author = {Silberschatz, Abraham and Korth, Henry F. and Sudarshan, S.},
  biburl = {https://www.bibsonomy.org/bibtex/2f918632bd9810e92303843bba065d27d/voj},
  edition = 6,
  interhash = {f7b2016b63d72e5cfd3bc55af1deedbb},
  intrahash = {f918632bd9810e92303843bba065d27d},
  keywords = {databases textbook},
  publisher = {McGraw-Hill},
  timestamp = {2010-07-18T14:00:17.000+0200},
  title = {Database system concepts},
  url = {http://www.db-book.com/},
  year = 2010
}

@ARTICLE{10106050,
  author={Zhao, Xinyang and Zhou, Xuanhe and Li, Guoliang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Automatic Database Knob Tuning: A Survey}, 
  year={2023},
  volume={35},
  number={12},
  pages={12470-12490},
  keywords={Tuning;Databases;Feature extraction;Costs;Measurement;Data models;Task analysis;Knob tuning;machine learning;knob selection;feature selection;tuning methods},
  doi={10.1109/TKDE.2023.3266893}}

@manual{PSQLManual,
  title        = "PostgreSQL 17.0 Documentation",
  author       = "{The PostgreSQL Global Development Group}",
  organization = "The PostgreSQL Global Development Group",
  year         = 2024
}

@manual{MySQLManual,
    title="MySQL 8.0 Reference Manual",
    author="{Oracle}",
    organization="Oracle",
    year=2024
}

@INPROCEEDINGS{9784588,
  author={Khosla, Cherry and Saini, Baljit Singh},
  booktitle={2021 International Conference on Computing Sciences (ICCS)}, 
  title={On a Way Together - Database and Machine Learning for Performance Tuning}, 
  year={2021},
  volume={},
  number={},
  pages={123-128},
  keywords={Autonomous systems;Machine learning;Database systems;Tuning;Database Tuning;Machine Learning;Tuning methods;Performance tuning},
  doi={10.1109/ICCS54944.2021.00032}}

@inproceedings{andreev2003performance,
  author    = {Vladimir Andreev},
  title     = {Performance Tuning Method},
  booktitle = {Vortragsband zur 16. Deutschen ORACLE-Anwenderkonferenz},
  year      = {2003},
  isbn      = {3-928490-13-3},
  issn      = {0939-1541},
  publisher = {Deutsche ORACLE-Anwendergruppe (DOAG)}
}

@INPROCEEDINGS{10090597,
  author={Wang, Junhai and Liu, Liqiang and Li, Cuixia},
  booktitle={2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)}, 
  title={A Survey of Database Knob Tuning with Machine Learning}, 
  year={2023},
  volume={},
  number={},
  pages={1545-1550},
  keywords={Electrical engineering;Deep learning;Machine learning algorithms;Reinforcement learning;Big Data;Database systems;Bayes methods;DBMS;knob tuning;Bayesian Optimization;Reinforcement Learning;Deep Learning},
  doi={10.1109/EEBDA56825.2023.10090597}}

@inproceedings{10.5555/3488733.3488749,
author = {Kanellis, Konstantinos and Alagappan, Ramnatthan and Venkataraman, Shivaram},
title = {Too many knobs to tune? towards faster database tuning by pre-selecting important knobs},
year = {2020},
publisher = {USENIX Association},
address = {USA},
abstract = {To achieve high performance, recent research has shown that it is important to automatically tune the configuration knobs present in database systems. However, as database systems usually have 100s of knobs, auto-tuning frameworks spend a significant amount of time exploring the large configuration space and need to repeat this as workloads change. Given this challenge, we ask a more fundamental question of how many knobs do we need to tune in order to achieve good performance. Surprisingly, we find that with YCSB workload-A on Cassandra, tuning just five knobs can achieve 99\% of the performance achieved by the best configuration that is obtained by tuning many knobs. We also show that our results hold across workloads and applies to other systems like PostgreSQL, motivating the need for tools that can automatically filter out the knobs that need to be tuned. Based on our results, we propose an initial design for accelerating auto-tuners and detail some future research directions.},
booktitle = {Proceedings of the 12th USENIX Conference on Hot Topics in Storage and File Systems},
articleno = {16},
numpages = {1},
series = {HotStorage '20}
}

@misc{PGTune2014, 
author = {Alexey Vasiliev},
year= 2014,
title={PgTune - Tuning PostgreSQL config by your hardware},
howpublished={\url{https://leopard.in.ua/2014/03/24/pgtune-for-postgresql}},
note         = "Accessed: 2024-11-03"
}

@INPROCEEDINGS{SARD2008,
  author={Debnath, Biplob K. and Lilja, David J. and Mokbel, Mohamed F.},
  booktitle={2008 IEEE 24th International Conference on Data Engineering Workshop}, 
  title={SARD: A statistical approach for ranking database tuning parameters}, 
  year={2008},
  volume={},
  number={},
  pages={11-18},
  keywords={Costs;System performance;Cities and towns;Thumb;Design methodology;Database systems;Computer science;Relational databases;Information technology;Personnel},
  doi={10.1109/ICDEW.2008.4498279}}

@INPROCEEDINGS{lasso2016,
  author={Muthukrishnan, R and Rohini, R},
  booktitle={2016 IEEE International Conference on Advances in Computer Applications (ICACA)}, 
  title={LASSO: A feature selection technique in predictive modeling for machine learning}, 
  year={2016},
  volume={},
  number={},
  pages={18-20},
  keywords={Computational modeling;Predictive models;Regression analysis;Input variables;Data models;Conferences;Computer applications;LASSO;Ridge regression;OLS;R software},
  doi={10.1109/ICACA.2016.7887916}}

@INPROCEEDINGS{ML,
  author={N, Thomas Rincy and Gupta, Roopam},
  booktitle={2020 IEEE International Students' Conference on Electrical,Electronics and Computer Science (SCEECS)}, 
  title={A Survey on Machine Learning Approaches and Its Techniques:}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Machine learning algorithms;Supervised learning;Machine learning;Reinforcement learning;Semisupervised learning;Predictive models;Unsupervised learning;Machine learning;Supervised learning;Unsupervised learning;Semi-supervised learning;Reinforcement Learning.},
  doi={10.1109/SCEECS48394.2020.190}}

@book{DBS7ed2016,
    author={Elmasri,Ramez and Navathe, Shamkant},
    year={2016},
    title={Fundamentals of Database Systems},
    edition={7},
    publisher={Pearson}
}

@INPROCEEDINGS{9898094,
  author={Fang, Xiang and Zou, Yi and Fang, Yange and Tang, Zhen and Li, Hui and Wang, Wei},
  booktitle={2022 IEEE International Conference on Joint Cloud Computing (JCC)}, 
  title={A Query-Level Distributed Database Tuning System with Machine Learning}, 
  year={2022},
  volume={},
  number={},
  pages={29-36},
  keywords={Cloud computing;Machine learning algorithms;Distributed databases;Machine learning;Manuals;Database systems;Tuning;query-level;knob tuning;distributed database;machine learning},
  doi={10.1109/JCC56315.2022.00012}}

@article{10.14778/3352063.3352129,
author = {Li, Guoliang and Zhou, Xuanhe and Li, Shifu and Gao, Bo},
title = {QTune: a query-aware database tuning system with deep reinforcement learning},
year = {2019},
issue_date = {August 2019},
publisher = {VLDB Endowment},
volume = {12},
number = {12},
issn = {2150-8097},
url = {https://doi.org/10.14778/3352063.3352129},
doi = {10.14778/3352063.3352129},
abstract = {Database knob tuning is important to achieve high performance (e.g., high throughput and low latency). However, knob tuning is an NP-hard problem and existing methods have several limitations. First, DBAs cannot tune a lot of database instances on different environments (e.g., different database vendors). Second, traditional machine-learning methods either cannot find good configurations or rely on a lot of high-quality training examples which are rather hard to obtain. Third, they only support coarse-grained tuning (e.g., workload-level tuning) but cannot provide fine-grained tuning (e.g., query-level tuning).To address these problems, we propose a query-aware database tuning system QTune with a deep reinforcement learning (DRL) model, which can efficiently and effectively tune the database configurations. QTune first featurizes the SQL queries by considering rich features of the SQL queries. Then QTune feeds the query features into the DRL model to choose suitable configurations. We propose a Double-State Deep Deterministic Policy Gradient (DS-DDPG) model to enable query-aware database configuration tuning, which utilizes the actor-critic networks to tune the database configurations based on both the query vector and database states. QTune provides three database tuning granularities: query-level, workload-level, and cluster-level tuning. We deployed our techniques onto three real database systems, and experimental results show that QTune achieves high performance and outperforms the state-of-the-art tuning methods.},
journal = {Proc. VLDB Endow.},
month = aug,
pages = {2118–2130},
numpages = {13}
}

@ARTICLE{8086184,
  author={Zhang, Mingyi and Martin, Patrick and Powley, Wendy and Chen, Jianjun},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={Workload Management in Database Management Systems: A Taxonomy}, 
  year={2018},
  volume={30},
  number={7},
  pages={1386-1402},
  keywords={Business;Servers;Database systems;Taxonomy;Process control;Monitoring;Taxonomy;workload management;database management systems},
  doi={10.1109/TKDE.2017.2767044}}

@misc{ioffe2015batchnormalizationacceleratingdeep,
      title={Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}, 
      author={Sergey Ioffe and Christian Szegedy},
      year={2015},
      eprint={1502.03167},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1502.03167}, 
}

@misc{agarap2019deeplearningusingrectified,
      title={Deep Learning using Rectified Linear Units (ReLU)}, 
      author={Abien Fred Agarap},
      year={2019},
      eprint={1803.08375},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1803.08375}, 
}

@book{ModernDatabaseManagement,
author = {Hoffer, Jeff and Venkataraman, Ramesh and Topi, Heikki},
title = {Modern Database Management},
year = {2015},
isbn = {1292101857},
publisher = {Pearson},
address = {USA},
edition = {12th}
}

@misc{ibmdb2performance,
    author = {Rees, Steve and Rech, Thomas and Depper, Olaf and Sing, Naveen K and Shen, Gang and Melnyk, Roman B.},
    title  = {Best practices: Tuning and monitoring database system performance},
    year   = {2013},
    month = {July},
    howpublished={IBM Information Management}
}

@book{AI-AMA,
    author = {Russel, Stuart and Norvig, Peter},
    title = {Artifical Intelligence: A Modern Apporach},
    publisher = {Prentice Hall},
    year = {2010},
    edition = {3},
    address = {England}
}

@inproceedings{DL,
    author = {Sarker, Iqbal H.},
    booktitle = {SN Computer Science},
    title = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
    year = {2021},
    volume={2},
    number={420},
    doi = {https://doi.org/10.1007/s42979-021-00815-1}
}

@inproceedings{ibmdb2kb,
    author = {Kwan, Eva and Lightstone, Sam and Storm, Adam and Wu, Leanne},
    title = {Automatic Configuration for IBM DB2 Universal Database},
    booktitle = {Performance Technical Report},
    year = {2002}
}